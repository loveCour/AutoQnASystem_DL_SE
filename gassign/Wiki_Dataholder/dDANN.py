import tensorflow as tf
import Wiki_Dataholder
import numpy

import QA_Model

from utils import *
from layers import *

from tensorflow.python.framework import ops
from flip_gradient import flip_gradient

import codecs

class FlipGradientBuilder(object):
    def __init__(self):
        self.num_calls = 0

    def __call__(self, x, l=1.0):
        grad_name = "FlipGradient%d" % self.num_calls

        @ops.RegisterGradient(grad_name)
        def _flip_gradients(op, grad):
            return [tf.negative(grad) * l]

        g = tf.get_default_graph()
        with g.gradient_override_map({"Identity": grad_name}):
            y = tf.identity(x)

        self.num_calls += 1
        return y


class One_Model:

    def get_variables_with_name(self, name, train_only=True, printable=False):
        """Get variable list by a given name scope.
        Examples
        ---------
        >>> dense_vars = tl.layers.get_variable_with_name('dense', True, True)
        """
        print("  [*] geting variables with %s" % name)
        # tvar = tf.trainable_variables() if train_only else tf.all_variables()
        if train_only:
            t_vars = tf.trainable_variables()
        else:
            try:  # TF1.0
                t_vars = tf.global_variables()
            except:  # TF0.12
                t_vars = tf.all_variables()

        d_vars = [var for var in t_vars if name in var.name]
        if printable:
            for idx, v in enumerate(d_vars):
                print("  got {:3}: {:15}   {}".format(idx, v.name, str(v.get_shape())))
        return d_vars

    def __init__(self):
        """
        Source Data: auto generated QA dataset with Wikipedia Korea
        Target Data: Exobrain QA Dataset (generated by human)

        pre-process program: KKMA
        word2vec: pre-trained with Wikipedia korea corpus

        Author: Sanghyeon Cho (delosycho@gmail.com)
        """

        self.Word_Embedding_Dimension = 200

        """
        korean embedding
        """
        word2vec_kor = codecs.open('kor_word2vec_200d', 'r', 'utf-8')
        self.kor_words = []
        self.kor_vectors = []

        arr = []
        for i in range(self.Word_Embedding_Dimension):
            pm = 1

            if i % 2 == 0:
                pm = -1

            arr.append(0.002 * pm * i)
        self.kor_words.append('#END')
        self.kor_vectors.append(arr)

        arr = []
        for i in range(self.Word_Embedding_Dimension):
            pm = 1

            if i % 2 == 0:
                pm = 0.1
            elif i % 3 == 0:
                pm = -1

            arr.append(0.1 * pm)
        self.kor_words.append('#START')
        self.kor_vectors.append(arr)

        for line in word2vec_kor:
            tokens = line.split('\t')
            self.kor_words.append(tokens.pop(0))
            self.kor_vectors.append(tokens)

        print(self.kor_words[0])
        print(self.kor_words[1])

        self.kor_dictionary = numpy.array(self.kor_words)
        self.word2vec_arg_index = self.kor_dictionary.argsort()
        self.kor_dictionary.sort()

        temp_arr = numpy.zeros(shape=[len(self.kor_dictionary), self.Word_Embedding_Dimension], dtype=numpy.float32)
        for i in range(len(self.kor_dictionary)):
            index = self.word2vec_arg_index[i]

            for j in range(self.Word_Embedding_Dimension):
                temp_arr[i, j] = self.kor_vectors[index][j]

        ################### word2vec tensor###########
        self.word_embedding_kor = tf.get_variable(
            "word2vec_kor", initializer=tf.constant(temp_arr, dtype=tf.float32), trainable=False)

        del self.kor_vectors
        temp_arr = None
        #memory deallocation

        #################################
        self.l = tf.placeholder(tf.float32, [], name='l')  # Gradient reversal scaler

        self.Dataset = Wiki_Dataholder.Data_holder()

        self.Y_start_source = tf.placeholder(dtype=tf.float32, shape=[None, None])
        self.Y_end_source = tf.placeholder(dtype=tf.float32, shape=[None, None])

        self.Y_start_target = tf.placeholder(dtype=tf.float32, shape=[None, None])
        self.Y_end_target = tf.placeholder(dtype=tf.float32, shape=[None, None])

        self.X_P_source = tf.placeholder(dtype=tf.int32, shape=[None, None])
        self.X_Q_source = tf.placeholder(dtype=tf.int32, shape=[None, None])

        self.X_P_target = tf.placeholder(dtype=tf.int32, shape=[None, None])
        self.X_Q_target = tf.placeholder(dtype=tf.int32, shape=[None, None])

        self.l = tf.placeholder(tf.float32, [])

        self.kor_vocab_size = self.kor_dictionary.shape[0]

        self.class_label = tf.placeholder(dtype=tf.float32, shape=[None, None])
        self.domain_label = tf.placeholder(dtype=tf.float32, shape=[None, None])

        self.p_length = self.Dataset.P_Length
        self.q_length = self.Dataset.Q_Length

        self.hidden_size = 200
        self.keep_prob = 0.95

        self.attention_hidden_size = 400
        self.batch_size = 64
        self.max_decoder_length = 50

        #self.char_mat = tf.get_variable(
        #    "char_mat", initializer=tf.constant(char_mat, dtype=tf.float32))

        #QA Model: BiDAF(Bidirectional Attention Flow)
        self.BiDAF_model = QA_Model.BIDAF()
        self.AOA_model = QA_Model.AOA_Reader()
        self.QANET_model = QA_Model.QA_Net(self.X_P_source, self.X_Q_source, self.p_length, self.q_length)
        self.SANET_model = QA_Model.SA_NET()

    def Discriminator(self, dis_inp, name, reuse=False):
        with tf.variable_scope(name) as scope:
            h1 = tf.contrib.layers.fully_connected(
                inputs=dis_inp,
                num_outputs=256,
                activation_fn=tf.nn.sigmoid,
                weights_initializer=tf.contrib.layers.xavier_initializer(),
                weights_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0004),
                biases_initializer=tf.constant_initializer(1e-04),
                scope="FC1",
                reuse=reuse
            )

            h2 = tf.contrib.layers.fully_connected(
                inputs=h1,
                num_outputs=512,
                activation_fn=tf.nn.sigmoid,
                weights_initializer=tf.contrib.layers.xavier_initializer(),
                weights_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0004),
                biases_initializer=tf.constant_initializer(1e-04),
                scope="FC2",
                reuse=reuse
            )

            h3 = tf.contrib.layers.fully_connected(
                inputs=h2,
                num_outputs=1024,
                activation_fn=tf.nn.sigmoid,
                weights_initializer=tf.contrib.layers.xavier_initializer(),
                weights_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0004),
                biases_initializer=tf.constant_initializer(1e-04),
                scope="FC3",
                reuse=reuse
            )

            h = tf.contrib.layers.fully_connected(
                inputs=h3,
                num_outputs=1,
                activation_fn=tf.nn.sigmoid,
                weights_initializer=tf.contrib.layers.xavier_initializer(),
                weights_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0004),
                biases_initializer=tf.constant_initializer(1e-04),
                scope="FC_",
                reuse=reuse
            )

            return h

    def context_encoder(self, inp, reuse=False):
        with tf.variable_scope("Input_Embedding_Layer"):
            """
            ch_emb = tf.reshape(tf.nn.embedding_lookup(
                self.char_mat, self.ch), [N * PL, CL, dc])
            qh_emb = tf.reshape(tf.nn.embedding_lookup(
                self.char_mat, self.qh), [N * QL, CL, dc])
            ch_emb = tf.nn.dropout(ch_emb, 1.0 - 0.5 * self.dropout)
            qh_emb = tf.nn.dropout(qh_emb, 1.0 - 0.5 * self.dropout)

            # Bidaf style conv-highway encoder
            ch_emb = conv(ch_emb, d,
                          bias=True, activation=tf.nn.relu, kernel_size=5, name="char_conv", reuse=None)
            qh_emb = conv(qh_emb, d,
                          bias=True, activation=tf.nn.relu, kernel_size=5, name="char_conv", reuse=True)

            ch_emb = tf.reduce_max(ch_emb, axis=1)
            qh_emb = tf.reduce_max(qh_emb, axis=1)

            ch_emb = tf.reshape(ch_emb, [N, PL, ch_emb.shape[-1]])
            qh_emb = tf.reshape(qh_emb, [N, QL, ch_emb.shape[-1]])

            c_emb = tf.nn.dropout(tf.nn.embedding_lookup(self.word_mat, self.c), 1.0 - self.dropout)
            q_emb = tf.nn.dropout(tf.nn.embedding_lookup(self.word_mat, self.q), 1.0 - self.dropout)

            c_emb = tf.concat([c_emb, ch_emb], axis=2)
            q_emb = tf.concat([q_emb, qh_emb], axis=2)
            """
        with tf.variable_scope("context_encoder_embedding", reuse=reuse) as scope:
            encoder_lookup_inputs = tf.nn.embedding_lookup(self.word_embedding_kor, inp)

            cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size)
            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob=self.keep_prob,
                                                    output_keep_prob=self.keep_prob)
            cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size)
            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob=self.keep_prob,
                                                    output_keep_prob=self.keep_prob)

            encoder_lookup_inputs = tf.nn.embedding_lookup(self.word_embedding_kor, inp)

            (fw_outputs, bw_outputs), (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(
                cell_fw=cell_fw, cell_bw=cell_bw, inputs=encoder_lookup_inputs,
                sequence_length=seq_length(encoder_lookup_inputs), dtype=tf.float32, time_major=False)

            context = tf.concat([fw_outputs, bw_outputs], -1)
            context_state = tf.concat([fw_state[-1], bw_state[-1]], -1)

            context = highway(context, size=296, scope='highway', dropout=0.1, reuse=reuse)
            """
            for i in range(2):
                encoder_lookup_inputs = Highway_Network_Fullyconnceted(encoder_lookup_inputs,
                                                                       dropout=0.1, name='highway' + str(i),
                                                                       padding=True, size=100,
                                                                       activation=tf.sigmoid, reuse=reuse)
            """

            scope.reuse_variables()

            return context, context_state

    def embedding_loockup(self, inp, reuse=False):
        with tf.variable_scope("context_encoder_embedding_only", reuse=reuse) as scope:
            encoder_lookup_inputs = tf.nn.embedding_lookup(self.word_embedding_kor, inp)

            return encoder_lookup_inputs

    def training_prediction_index(self, training_epoch, is_continue, is_DANN, source, target,
                                  Model=0, gradient=True, l2_norm=3e-7):
        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)

        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
            total_loss = 0

            context_p_source, context_p_source_state = self.context_encoder(self.X_P_source)
            context_q_source, context_q_source_state = self.context_encoder(self.X_Q_source, reuse=True)

            context_p_target, context_p_target_state = self.context_encoder(self.X_P_target, reuse=True)
            context_q_target, context_q_target_state = self.context_encoder(self.X_Q_target, reuse=True)

            context_p_source_ = flip_gradient(context_p_source_state, self.l)
            context_q_source_ = flip_gradient(context_q_source_state, self.l)
            context_p_target_ = flip_gradient(context_p_target_state, self.l)
            context_q_target_ = flip_gradient(context_q_target_state, self.l)

            dann_loss = 0
            if is_DANN is True:
                dis1 = self.Discriminator(context_p_source_, name='Discriminator')
                dis2 = self.Discriminator(context_q_source_, name='Discriminator', reuse=True)
                dis3 = self.Discriminator(context_p_target_, name='Discriminator', reuse=True)
                dis4 = self.Discriminator(context_q_target_, name='Discriminator', reuse=True)

                dann_loss1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(dis1), logits=dis1)
                dann_loss2 = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(dis2), logits=dis2)
                dann_loss3 = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(dis3), logits=dis3)
                dann_loss4 = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(dis4), logits=dis4)

                if source is True:
                    dann_loss += dann_loss1 + dann_loss2
                if target is True:
                    dann_loss += dann_loss3 + dann_loss4

            if Model == 0:
                start_prediction_s, end_prediction_s = self.BiDAF_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)

                start_prediction_t, end_prediction_t = self.BiDAF_model.create_graph(
                    x_p=context_p_target, x_q=context_q_target, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False, reuse=True)
            elif Model == 1:
                start_prediction_s, end_prediction_s = self.QANET_model.create_graph(d=296,
                    c_emb=context_p_source, q_emb=context_q_source, p_length=self.p_length, q_length=self.q_length, reuse=False
                )

                start_prediction_t, end_prediction_t = self.QANET_model.create_graph(d=296,
                    c_emb=context_p_target, q_emb=context_q_target, reuse=True, p_length=self.p_length, q_length=self.q_length
                )
            elif Model == 2:
                start_prediction_s, end_prediction_s = self.SANET_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)

                start_prediction_t, end_prediction_t = self.SANET_model.create_graph(
                    x_p=context_p_target, x_q=context_q_target, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False, reuse=True)
            elif Model == 3:
                start_prediction_s, end_prediction_s = self.AOA_model.create_graph(x_p=context_p_source,
                                                                                   x_q=context_q_source,
                                                                                   p_length=self.p_length,
                                                                                   q_length=self.q_length)
                start_prediction_t, end_prediction_t = self.AOA_model.create_graph(x_p=context_p_target,
                                                                                   x_q=context_q_target,
                                                                                   p_length=self.p_length,
                                                                                   q_length=self.q_length, reuse=True)
            else:
                #default model
                start_prediction_s, end_prediction_s = self.BiDAF_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)

                start_prediction_t, end_prediction_t = self.BiDAF_model.create_graph(
                    x_p=context_p_target, x_q=context_q_target, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False, reuse=True)
                """
                start_prediction_s, end_prediction_s = self.QANET_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)

                start_prediction_t, end_prediction_t = self.QANET_model.create_graph(
                    x_p=context_p_target, x_q=context_q_target, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False, reuse=True)
                """
            class_loss_source = tf.nn.softmax_cross_entropy_with_logits\
                                    (labels=self.Y_start_source, logits=start_prediction_s) + \
                                tf.nn.softmax_cross_entropy_with_logits\
                                    (labels=self.Y_end_source, logits=end_prediction_s)

            class_loss_target = tf.nn.softmax_cross_entropy_with_logits \
                                    (labels=self.Y_start_target, logits=start_prediction_t) + \
                                tf.nn.softmax_cross_entropy_with_logits \
                                    (labels=self.Y_end_target, logits=end_prediction_t)

            class_loss = 0
            if source is True:
                class_loss += tf.reduce_mean(class_loss_source)
            if target is True:
                class_loss += tf.reduce_mean(class_loss_target)

            if is_DANN is True:
                total_loss += class_loss + dann_loss
            else:
                total_loss += class_loss

            regularizer = tf.contrib.layers.l2_regularizer(scale=3e-7)
            if l2_norm is not None:
                variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
                l2_loss = tf.contrib.layers.apply_regularization(regularizer, variables)
                total_loss += l2_loss

            global_step = tf.Variable(0, trainable=False)
            starter_learning_rate = 0.0003
            grad_clip = 5.0
            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,
                                                       1000, 0.96, staircase=True)

            if gradient is True:
                opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.8, beta2=0.999, epsilon=1e-7)
                grads = opt.compute_gradients(total_loss)
                gradients, variables = zip(*grads)
                capped_grads, _ = tf.clip_by_global_norm(
                    gradients, grad_clip)
                optimizer = opt.apply_gradients(
                    zip(capped_grads, variables), global_step=global_step)
            else:
                optimizer = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)

            sess.run(tf.initialize_all_variables())

            if is_continue:
                saver = tf.train.Saver()
                save_path = saver.restore(sess, 'D:\qa_data/Index/DMA_Net.ckpf')

            epo = 0

            while epo < training_epoch:
                epo += 1
                # self.dataset.Batch_Index = 0

                source_paragraph, source_question, source_start_label, source_stop_label, target_paragraph, \
                target_question, target_start_label, target_stop_label = self.Dataset.get_Next_Batch()

                #X_P, X_Q (target, source)
                training_feed_dict = {self.X_P_source: source_paragraph, self.X_Q_source: source_question,
                                      self.X_P_target: target_paragraph, self.X_Q_target: target_question,
                                      self.Y_start_source: source_start_label, self.Y_end_source: source_stop_label,
                                      self.Y_start_target: target_start_label, self.Y_end_target: target_stop_label,
                                      self.l: -1.0}

                _, loss_value = sess.run([optimizer, total_loss], feed_dict=training_feed_dict)
                print(epo, ',', ':', loss_value)

                if epo % 100 == 0:
                    saver = tf.train.Saver()
                    save_path = saver.save(sess, 'D:\qa_data/Index/DMA_Net.ckpf')
                    print('saved!')

            saver = tf.train.Saver()
            save_path = saver.save(sess, 'D:\qa_data/Index/DMA_Net.ckpf')

        return 0

    def check(self, is_BIDAF):
        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)

        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
            context_p_source, context_p_source_state = self.context_encoder(self.X_P_source)
            context_q_source, context_q_source_state = self.context_encoder(self.X_Q_source, reuse=True)

            enc = residual_conv_block(context_p_source, 3, 2, 3, 96, False, self.p_length, 8, 'res_block', True, 0.0, False, False)
            #inputs, num_blocks, num_conv_layer, kernel_size, reuse, num_filters, input_projection,
            #            seq_len, num_heads, name, is_training=True, dropout=0.0, bias=True

            if is_BIDAF is True:
                0
                #start_prediction_s, end_prediction_s = self.BiDAF_model.create_graph(
                #    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                #    context_encoding=False)
            else:
                0
                #start_prediction_s, end_prediction_s = self.QANET_model.create_graph(
                #    c_emb=context_p_source, q_emb=context_q_source, reuse=True, p_length=self.p_length, q_length=self.q_length
                #)

            sess.run(tf.initialize_all_variables())

            #saver = tf.train.Saver()
            #save_path = saver.restore(sess, 'D:\qa_data/Index/DMA_Net.ckpf')

            correct = 0
            wrong = 0
            correct2 = 0
            wrong2 = 0

            paragraph, question, start_label, stop_label, _ = self.Dataset.get_Test_Source_Batch()

            training_feed_dict = {self.X_P_source: paragraph, self.X_Q_source: question,
                                  self.Y_start_source: start_label, self.Y_end_source: stop_label,
                                  self.l: -1.0}

            result_start = numpy.array(sess.run(context_p_source, feed_dict=training_feed_dict))
            #result_stop = numpy.array(sess.run(end_prediction_s, feed_dict=training_feed_dict))

            print(result_start.shape)
            #print(result_start[0][0])

    def test_source(self, Model, display):
        result_file = open('result_file_true.txt', 'w', encoding='utf-8')
        result_file_ = open('result_file_false.txt', 'w', encoding='utf-8')

        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)

        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
            context_p_source, context_p_source_state = self.context_encoder(self.X_P_source)
            context_q_source, context_q_source_state = self.context_encoder(self.X_Q_source, reuse=True)

            if Model == 0:
                start_prediction_s, end_prediction_s = self.BiDAF_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)

            elif Model == 1:
                start_prediction_s, end_prediction_s = self.QANET_model.create_graph(d=296,
                    c_emb=context_p_source, q_emb=context_q_source, p_length=self.p_length, q_length=self.q_length, reuse=False
                )

            elif Model == 2:
                start_prediction_s, end_prediction_s = self.SANET_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)
            elif Model == 3:
                start_prediction_s, end_prediction_s = self.AOA_model.create_graph(x_p=context_p_source,
                                                                                   x_q=context_q_source,
                                                                                   p_length=self.p_length,
                                                                                   q_length=self.q_length)
            else:
                #default model
                start_prediction_s, end_prediction_s = self.BiDAF_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)

                """
                start_prediction_s, end_prediction_s = self.QANET_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)

                start_prediction_t, end_prediction_t = self.QANET_model.create_graph(
                    x_p=context_p_target, x_q=context_q_target, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False, reuse=True)
                """

            sess.run(tf.initialize_all_variables())

            saver = tf.train.Saver()
            save_path = saver.restore(sess, 'D:\qa_data/Index/DMA_Net.ckpf')

            correct = 0
            wrong = 0
            correct2 = 0
            wrong2 = 0

            while True:
                paragraph, question, start_label, stop_label, np_range = self.Dataset.get_Test_Source_Batch()

                if paragraph is 0:
                    break
                else:
                    print(self.Dataset.Test_Batch_index)

                training_feed_dict = {self.X_P_source: paragraph, self.X_Q_source: question,
                                      self.Y_start_source: start_label, self.Y_end_source: stop_label,
                                      self.l: -1.0}

                result_start = numpy.array(sess.run(start_prediction_s, feed_dict=training_feed_dict))
                result_stop = numpy.array(sess.run(end_prediction_s, feed_dict=training_feed_dict))

                for i in range(result_start.shape[0]):
                    true1 = False
                    true2 = False

                    max_v = -999
                    max_index = -1

                    for j in range(result_start.shape[1]):
                        if max_v < result_start[i, j]:
                            max_v = result_start[i, j]
                            max_index = j

                    start_index = max_index

                    if start_label[i, max_index] > 0.5:
                        correct += 1
                        true1 = True

                    else:
                        wrong += 1

                    max_v = -999
                    max_index = -1

                    for j in range(result_stop.shape[1]):
                        if max_v < result_stop[i, j]:
                            max_v = result_stop[i, j]
                            max_index = j

                    if stop_label[i, max_index] > 0.5:
                        correct2 += 1
                        true2 = True
                    else:
                        wrong2 += 1
                        if display is True:
                            idx = np_range[i]
                            #print(idx)
                            #print('start_result, start_label:', start_index, self.Dataset.Labels_start[idx])
                            #print('stop_result, stop_label:', max_index, self.Dataset.Labels_stop[idx])
                            #print(start_label[i], '\n', stop_label[i])
                            #print(self.Dataset.Paragraphs[idx])
                            TK = str(self.Dataset.Paragraphs_[idx]).split(' ')
                            #print('max, answer', TK[start_index], TK[max_index])
                            #print('max, answer', TK[self.Dataset.Labels_start[idx]], TK[self.Dataset.Labels_stop[idx]])

                            #print(self.Dataset.Questions[idx])

                    if true1 is True and true2 is True:
                        idx = np_range[i]
                        TK = str(self.Dataset.Paragraphs_[idx]).split(' ')

                        result_file.write(self.Dataset.Questions[idx])

                        for i in range(len(TK)):
                            if i % 10 == 0:
                                result_file.write('\n')
                            if self.Dataset.Labels_start[idx] == i:
                                result_file.write('(')
                            if start_index == i:
                                result_file.write('<')
                            result_file.write(TK[i])
                            if max_index == i:
                                result_file.write('>')
                            if self.Dataset.Labels_stop[idx] == i:
                                result_file.write(')')
                            result_file.write(' ')

                        result_file.write('\n------------------------\n\n')
                    else:
                        idx = np_range[i]
                        TK = str(self.Dataset.Paragraphs_[idx]).split(' ')
                        result_file_.write(self.Dataset.Questions[idx])

                        for i in range(len(TK)):
                            if i % 10 == 0:
                                result_file_.write('\n')
                            if self.Dataset.Labels_start[idx] == i:
                                result_file_.write('(')
                            if start_index == i:
                                result_file_.write('<')
                            result_file_.write(TK[i])
                            if max_index == i:
                                result_file_.write('>')
                            if self.Dataset.Labels_stop[idx] == i:
                                result_file_.write(')')
                            result_file_.write(' ')

                        result_file_.write('\n------------------------\n\n')

        print(correct, '/', (correct + wrong))
        print(correct2, '/', (correct2 + wrong2))
        result_file.close()
    def test_target(self, Model, display):
        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)

        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
            context_p_source, context_p_source_state = self.context_encoder(self.X_P_source)
            context_q_source, context_q_source_state = self.context_encoder(self.X_Q_source, reuse=True)

            if Model == 0:
                start_prediction_s, end_prediction_s = self.BiDAF_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)

            elif Model == 1:
                start_prediction_s, end_prediction_s = self.QANET_model.create_graph(d=600,
                                                                                     c_emb=context_p_source,
                                                                                     q_emb=context_q_source,
                                                                                     p_length=self.p_length,
                                                                                     q_length=self.q_length, reuse=False
                                                                                     )

            elif Model == 2:
                start_prediction_s, end_prediction_s, p_loss_s = self.SANET_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)
            elif Model == 3:
                start_prediction_s, end_prediction_s = self.AOA_model.create_graph(x_p=context_p_source,
                                                                                   x_q=context_q_source,
                                                                                   p_length=self.p_length,
                                                                                   q_length=self.q_length)
            else:
                # default model
                start_prediction_s, end_prediction_s = self.BiDAF_model.create_graph(
                    x_p=context_p_source, x_q=context_q_source, p_length=self.p_length, q_length=self.q_length,
                    context_encoding=False)

            sess.run(tf.initialize_all_variables())

            saver = tf.train.Saver()
            save_path = saver.restore(sess, 'D:\qa_data/Index/DMA_Net.ckpf')

            correct = 0
            wrong = 0
            correct2 = 0
            wrong2 = 0

            while True:
                paragraph, question, start_label, stop_label, np_range = self.Dataset.get_Test_Target_Batch()

                if paragraph is 0:
                    break
                else:
                    print(self.Dataset.Test_Batch_index2)

                training_feed_dict = {self.X_P_source: paragraph, self.X_Q_source: question,
                                      self.Y_start_source: start_label, self.Y_end_source: stop_label,
                                      self.l: -1.0}

                result_start = numpy.array(sess.run(start_prediction_s, feed_dict=training_feed_dict), dtype=numpy.float32)
                result_stop = numpy.array(sess.run(end_prediction_s, feed_dict=training_feed_dict), dtype=numpy.float32)

                for i in range(result_start.shape[0]):
                    max_v = -999
                    max_index = -1

                    for j in range(result_start.shape[1]):
                        if max_v < result_start[i, j]:
                            max_v = result_start[i, j]
                            max_index = j

                    start_index = max_index

                    if start_label[i, max_index] > 0.5:
                        correct += 1
                    else:
                        wrong += 1

                    max_v = -999
                    max_index = -1

                    for j in range(result_stop.shape[1]):
                        if max_v < result_stop[i, j]:
                            max_v = result_stop[i, j]
                            max_index = j

                    if stop_label[i, max_index] > 0.5:
                        correct2 += 1

                    else:
                        wrong2 += 1
                        if display is True:
                            idx = np_range[i]
                            print(idx)
                            print('start_result, start_label:', start_index, self.Dataset.exo_Labels_start[idx])
                            print('stop_result, stop_label:', max_index, self.Dataset.exo_Labels_stop[idx])
                            #print(start_label[i], stop_label[i])
                            print(self.Dataset.exo_Paragraphs[idx])
                            TK = str(self.Dataset.exo_Paragraphs[idx]).split(' ')
                            print(TK[start_index], TK[max_index])
                            print(TK[self.Dataset.exo_Labels_start[idx]], TK[self.Dataset.exo_Labels_stop[idx]])
                            print(self.Dataset.exo_Questions[idx])
                            input()

        print(correct, '/', (correct + wrong))
        print(correct2, '/', (correct2 + wrong2))
